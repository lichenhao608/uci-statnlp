{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from test_trigram import learn_trigram\n",
    "from test_bigram import learn_bigram\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "brown\n",
      "brown  read. train: 39802 dev: 8437 test: 8533\n",
      "vocab: 41746\n",
      "train: 1513.8018008490042\n",
      "dev  : 1737.5445705338257\n",
      "test : 1758.248804766443\n",
      "sample:  Baker\n",
      "sample:  have room They there event thanks bad his lobes making Treasury\n",
      "sample:  pirates South\n",
      "-----------------------\n",
      "reuters\n",
      "reuters  read. train: 38183 dev: 8083 test: 8199\n",
      "vocab: 35989\n",
      "train: 1466.8721485743788\n",
      "dev  : 1580.9102794282078\n",
      "test : 1576.8543845321956\n",
      "sample:  753 cts Saunders from February on Lanka week said 000 dlrs to 29 the MINING of say\n",
      "sample:  mln now Japan Canada pct 100 resolved 70 fourth which Change heavy UNIT has but barrels planned premium TO will approved that the CUTS Iran the same ministry of Coffee STOCKS is from\n",
      "sample:  government Prudential\n",
      "-----------------------\n",
      "gutenberg\n",
      "gutenberg  read. train: 68767 dev: 14667 test: 14861\n",
      "vocab: 43736\n",
      "train: 981.368830109398\n",
      "dev  : 1060.5363793834274\n",
      "test : 1035.7794090182354\n",
      "sample:  not little who laughed part to sneeringly yet Therefore were them While their mine 26 his deserving as Grace safe expected well Let and Provoking gardeners the if the at Admiral and went Miss his 19 and to any 53 which which from across travel year set she found\n",
      "sample:  taste be fire The unto LORD when sake time Marianne robin of that should at coined her much and trowsers Henrietta some of hath\n",
      "sample:  pots hour good been brown unto or remained wealth were made quantity before opened be for\n",
      "-------------------------------\n",
      "x train\n",
      "                          brown         reuters       gutenberg \n",
      "          brown 1513.8018008490042 17443.539333228993 2459.1683279238387 \n",
      "        reuters 7216.010219096019 1466.8721485743788 12016.77483734755 \n",
      "      gutenberg 4130.0927500641965 44905.97604942636 981.368830109398 \n",
      "-------------------------------\n",
      "x dev\n",
      "                          brown         reuters       gutenberg \n",
      "          brown 1737.5445705338257 15038.135021896655 2311.5610963996223 \n",
      "        reuters 6534.226497330145 1580.9102794282078 10578.833578898077 \n",
      "      gutenberg 3778.554120391211 37095.37002620345 1060.5363793834274 \n",
      "-------------------------------\n",
      "x test\n",
      "                          brown         reuters       gutenberg \n",
      "          brown 1758.248804766443 15344.329772188432 2308.5358277923247 \n",
      "        reuters 6616.477351259757 1576.8543845321956 10561.680093366109 \n",
      "      gutenberg 3819.4033651494237 37900.87730669584 1035.7794090182354 \n"
     ]
    }
   ],
   "source": [
    "dnames = [\"brown\", \"reuters\", \"gutenberg\"]\n",
    "datas = []\n",
    "models = []\n",
    "# Learn the models for each of the domains, and evaluate it\n",
    "for dname in dnames:\n",
    "    print(\"-----------------------\")\n",
    "    print(dname)\n",
    "    data = read_texts(\"data/corpora.tar.gz\", dname)\n",
    "    datas.append(data)\n",
    "    model = learn_unigram(data)\n",
    "    models.append(model)\n",
    "# compute the perplexity of all pairs\n",
    "n = len(dnames)\n",
    "perp_dev = np.zeros((n,n))\n",
    "perp_test = np.zeros((n,n))\n",
    "perp_train = np.zeros((n,n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        perp_dev[i][j] = models[i].perplexity(datas[j].dev)\n",
    "        perp_test[i][j] = models[i].perplexity(datas[j].test)\n",
    "        perp_train[i][j] = models[i].perplexity(datas[j].train)\n",
    "\n",
    "print(\"-------------------------------\")\n",
    "print(\"x train\")\n",
    "print_table(perp_train, dnames, dnames, \"table-train.tex\")\n",
    "print(\"-------------------------------\")\n",
    "print(\"x dev\")\n",
    "print_table(perp_dev, dnames, dnames, \"table-dev.tex\")\n",
    "print(\"-------------------------------\")\n",
    "print(\"x test\")\n",
    "print_table(perp_test, dnames, dnames, \"table-test.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lm(base, smooth, mode='bi', comp_other=False):\n",
    "    if mode == 'bi':\n",
    "        learn_func = learn_bigram\n",
    "    elif mode == 'tri':\n",
    "        learn_func = learn_trigram\n",
    "        \n",
    "    dnames = [\"brown\", \"reuters\", \"gutenberg\"]\n",
    "    datas = []\n",
    "    models = {b:defaultdict(list) for b in base}\n",
    "    # Learn the models for each of the domains, and evaluate it\n",
    "    for dname in dnames:\n",
    "        print(\"-----------------------\")\n",
    "        print(dname)\n",
    "        data = read_texts(\"data/corpora.tar.gz\", dname)\n",
    "        datas.append(data)\n",
    "        \n",
    "        for b in base:\n",
    "            for s in smooth:\n",
    "                model = learn_func(data, gamma=b, smooth=s)\n",
    "                models[b][s].append(model)\n",
    "                \n",
    "    if comp_other:\n",
    "        # compute the perplexity of all pairs\n",
    "        n = len(dnames)\n",
    "        perp_dev = np.zeros((n,n))\n",
    "        perp_test = np.zeros((n,n))\n",
    "        perp_train = np.zeros((n,n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                perp_dev[i][j] = models[i].perplexity(datas[j].dev)\n",
    "                perp_test[i][j] = models[i].perplexity(datas[j].test)\n",
    "                perp_train[i][j] = models[i].perplexity(datas[j].train)\n",
    "\n",
    "        print(\"-------------------------------\")\n",
    "        print(\"x train\")\n",
    "        print_table(perp_train, dnames, dnames, \"table-train.tex\")\n",
    "        print(\"-------------------------------\")\n",
    "        print(\"x dev\")\n",
    "        print_table(perp_dev, dnames, dnames, \"table-dev.tex\")\n",
    "        print(\"-------------------------------\")\n",
    "        print(\"x test\")\n",
    "        print_table(perp_test, dnames, dnames, \"table-test.tex\")\n",
    "    \n",
    "    return models, datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = np.linspace(0.1, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "brown\n",
      "brown  read. train: 39802 dev: 8437 test: 8533\n",
      "-----------------------\n",
      "reuters\n",
      "reuters  read. train: 38183 dev: 8083 test: 8199\n",
      "-----------------------\n",
      "gutenberg\n",
      "gutenberg  read. train: 68767 dev: 14667 test: 14861\n"
     ]
    }
   ],
   "source": [
    "bimodels_0, datas = train_lm([0], smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: defaultdict(<class 'list'>, {0.1: [<lm.Bigram object at 0x0000025AFF482288>, <lm.Bigram object at 0x0000025AB0BB2548>, <lm.Bigram object at 0x0000025AFFCBCAC8>], 0.2: [<lm.Bigram object at 0x0000025AFF482D08>, <lm.Bigram object at 0x0000025AB0BB2448>, <lm.Bigram object at 0x0000025AFFC4FF48>], 0.30000000000000004: [<lm.Bigram object at 0x0000025AFF482308>, <lm.Bigram object at 0x0000025AB0BB2908>, <lm.Bigram object at 0x0000025AFFCBC948>], 0.4: [<lm.Bigram object at 0x0000025AFF482FC8>, <lm.Bigram object at 0x0000025AB0BB2C88>, <lm.Bigram object at 0x0000025AFFCBCE08>], 0.5: [<lm.Bigram object at 0x0000025AFF482B08>, <lm.Bigram object at 0x0000025AB0BB2048>, <lm.Bigram object at 0x0000025AFFCBC108>], 0.6: [<lm.Bigram object at 0x0000025AFF482DC8>, <lm.Bigram object at 0x0000025AB0BB2BC8>, <lm.Bigram object at 0x0000025AFFCBC608>], 0.7000000000000001: [<lm.Bigram object at 0x0000025AFF482808>, <lm.Bigram object at 0x0000025AB0BB2EC8>, <lm.Bigram object at 0x0000025AFFCBC648>], 0.8: [<lm.Bigram object at 0x0000025AFF482AC8>, <lm.Bigram object at 0x0000025AB0BB27C8>, <lm.Bigram object at 0x0000025AFFCBC308>], 0.9: [<lm.Bigram object at 0x0000025AFF4825C8>, <lm.Bigram object at 0x0000025AB0BB22C8>, <lm.Bigram object at 0x0000025AFFCBC8C8>], 1.0: [<lm.Bigram object at 0x0000025AFF4822C8>, <lm.Bigram object at 0x0000025AB0BB2A88>, <lm.Bigram object at 0x0000025AFE899588>]})}\n"
     ]
    }
   ],
   "source": [
    "print(bimodels_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp(base, smooth, datas, models):\n",
    "    # compute the perplexity of all pairs\n",
    "    bnum = len(base)\n",
    "    snum = len(smooth)\n",
    "    perp_dev = np.zeros((bnum, snum, 3,3))\n",
    "    perp_test = np.zeros((bnum, snum, 3,3))\n",
    "    perp_train = np.zeros((bnum, snum, 3,3))\n",
    "    \n",
    "    for bi, b in enumerate(base):\n",
    "        for si, s in enumerate(smooth):\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    perp_dev[bi,si,i,j] = models[b][s][i].perplexity(datas[j].dev)\n",
    "                    perp_test[bi,si,i,j] = models[b][s][i].perplexity(datas[j].test)\n",
    "                    perp_train[bi,si,i,j] = models[b][s][i].perplexity(datas[j].train)\n",
    "\n",
    "#     print(\"-------------------------------\")\n",
    "#     print(\"x train\")\n",
    "#     print_table(perp_train, dnames, dnames, \"table-train.tex\")\n",
    "#     print(\"-------------------------------\")\n",
    "#     print(\"x dev\")\n",
    "#     print_table(perp_dev, dnames, dnames, \"table-dev.tex\")\n",
    "#     print(\"-------------------------------\")\n",
    "#     print(\"x test\")\n",
    "#     print_table(perp_test, dnames, dnames, \"table-test.tex\")\n",
    "    return perp_dev, perp_test, perp_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "perp = comp([0], smooth, datas, bimodels_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3794.83356135 4795.40425634 5577.73281865 6241.02797914 6825.44963312\n",
      " 7352.22512987 7834.28373878 8280.23747099 8696.18873248 9086.6623328 ]\n"
     ]
    }
   ],
   "source": [
    "print(perp[0][0,:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bi_smooth01_0.pickle', 'wb') as f:\n",
    "    pickle.dump(bimodels_0, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
